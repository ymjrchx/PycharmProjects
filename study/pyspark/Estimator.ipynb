{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RFormula Estimator来创建一个特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-python\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+-----------+-------+\n",
      "|origin|model|hour|temperature|arrival|\n",
      "+------+-----+----+-----------+-------+\n",
      "|   SFO| B737|  18|       95.1|   late|\n",
      "|   SEA| A319|   5|       65.7| ontime|\n",
      "|   LAX| B747|  15|       31.5|   late|\n",
      "|   ATL| A319|  14|       40.5|   late|\n",
      "+------+-----+----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arrival_data = spark.createDataFrame(\n",
    "[\n",
    "(\"SFO\", \"B737\", 18, 95.1, \"late\"),\n",
    "(\"SEA\", \"A319\", 5, 65.7, \"ontime\"),\n",
    "(\"LAX\", \"B747\", 15, 31.5, \"late\"),\n",
    "(\"ATL\", \"A319\", 14, 40.5, \"late\")\n",
    "], [\"origin\", \"model\", \"hour\", \"temperature\", \"arrival\"])\n",
    "arrival_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+-----------+-------+------------------------------------------+\n",
      "|origin|model|hour|temperature|arrival|features                                  |\n",
      "+------+-----+----+-----------+-------+------------------------------------------+\n",
      "|SFO   |B737 |18  |95.1       |late   |[1.0,0.0,0.0,0.0,0.0,18.0,95.1,1.0,1711.8]|\n",
      "|SEA   |A319 |5   |65.7       |ontime |[0.0,1.0,0.0,1.0,0.0,5.0,65.7,0.0,328.5]  |\n",
      "|LAX   |B747 |15  |31.5       |late   |[0.0,0.0,0.0,0.0,1.0,15.0,31.5,1.0,472.5] |\n",
      "|ATL   |A319 |14  |40.5       |late   |[0.0,0.0,1.0,1.0,0.0,14.0,40.5,1.0,567.0] |\n",
      "+------+-----+----+-----------+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RFormula用于将数据中的字段通过R语言的Model Formulae转换成特征值，输出结果为一个特征向量和Double类型的label\n",
    "#关于R语言Model Formulae的介绍可参考：https://stat.ethz.ch/R-manual/Rdevel/library/stats/html/formula.html\n",
    "formula = RFormula(formula=\"arri~ . + hour:temperature\", featuresCol = \"features\", labelCol= \"label\")\n",
    "# 首先调用fit函数，它返回一个模型(model，类型为transformer),然后调用transform\n",
    "# output = formula.fit(arrival_data).transform(arrival_data)\n",
    "model = formula.fit(arrival_data)\n",
    "output = model.transform(arrival_data)\n",
    "output.select('*').show(truncate=False) # 在scala能显示label，这里没有？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用IDF estimator来计算每个单词的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                line|\n",
      "+---+--------------------+\n",
      "|  1|Spark is a unifie...|\n",
      "|  2|Spark is cool and...|\n",
      "|  3|There is a lot of...|\n",
      "|  4|mllib transformer...|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|                line|               words|        wordFreqVect|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|Spark is a unifie...|[spark, is, a, un...|(262144,[1461,747...|\n",
      "|  2|Spark is cool and...|[spark, is, cool,...|(262144,[8443,158...|\n",
      "|  3|There is a lot of...|[there, is, a, lo...|(262144,[3023,891...|\n",
      "|  4|mllib transformer...|[mllib, transform...|(262144,[91106,91...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n",
      "IDF_283fffce2c9a\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|                line|               words|        wordFreqVect|            features|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|Spark is a unifie...|[spark, is, a, un...|(262144,[1461,747...|(262144,[1461,747...|\n",
      "|  2|Spark is cool and...|[spark, is, cool,...|(262144,[8443,158...|(262144,[8443,158...|\n",
      "|  3|There is a lot of...|[there, is, a, lo...|(262144,[3023,891...|(262144,[3023,891...|\n",
      "|  4|mllib transformer...|[mllib, transform...|(262144,[91106,91...|(262144,[91106,91...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "text_data = spark.createDataFrame(\n",
    "[\n",
    "(1, \"Spark is a unified data analytics engine\"),\n",
    "(2, \"Spark is cool and it is fun to work with Spark\"),\n",
    "(3, \"There is a lot of exciting sessions at upcoming Spark summit\"),\n",
    "(4, \"mllib transformer estimator evaluator and pipelines\")\n",
    "],[\"id\", \"line\"])\n",
    "text_data.show()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"line\", outputCol=\"words\")\n",
    "\n",
    "# Tokenizer transformer的输出列是HashingTF的输入\n",
    "tf = HashingTF(inputCol=\"words\", outputCol=\"wordFreqVect\")\n",
    "tfResult = tf.transform(tokenizer.transform(text_data))\n",
    "tfResult.show()\n",
    "\n",
    "# HashingTF transformer的输出是IDF estimator的输入\n",
    "idf = IDF(inputCol=\"wordFreqVect\", outputCol=\"features\")\n",
    "\n",
    "# 因为IDF是一个estimator,所以调用fit函数\n",
    "idfModel = idf.fit(tfResult)\n",
    "print(idfModel)\n",
    "# 返回对象是一个模型（Model）, 它是类型Transformer\n",
    "weightedWords = idfModel.transform(tfResult)\n",
    "weightedWords.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[1461,7473,15889,110213,160735,227410,234657],[0.9162907318741551,0.9162907318741551,0.22314355131420976,0.9162907318741551,0.9162907318741551,0.5108256237659907,0.22314355131420976])                                                                                                     |\n",
      "|(262144,[8443,15889,34343,86175,91677,126466,205044,223619,234657],[0.9162907318741551,0.44628710262841953,0.9162907318741551,0.9162907318741551,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.44628710262841953])                                                  |\n",
      "|(262144,[3023,8916,9639,14250,15889,128231,166806,176964,180535,227410,234657],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.22314355131420976,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.5108256237659907,0.22314355131420976])|\n",
      "|(262144,[91106,91677,163638,165972,166537,191938],[0.9162907318741551,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.9162907318741551,0.9162907318741551])                                                                                                                              |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- line: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordFreqVect: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feature列包含一个向量用于每个单词的权重\n",
    "weightedWords.select(\"features\").show(truncate=False)\n",
    "weightedWords.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用StringIndexer estimator来对电影类型进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "| id| genre|genreIdx|\n",
      "+---+------+--------+\n",
      "|  1|Comedy|     0.0|\n",
      "|  2|Action|     1.0|\n",
      "|  3|Comedy|     0.0|\n",
      "|  4|Horror|     2.0|\n",
      "|  5|Action|     1.0|\n",
      "|  6|Comedy|     0.0|\n",
      "+---+------+--------+\n",
      "\n",
      "['Comedy', 'Action', 'Horror']\n",
      "+---+------+--------+\n",
      "| id| genre|genreIdx|\n",
      "+---+------+--------+\n",
      "|  3|Comedy|     0.0|\n",
      "|  1|Comedy|     0.0|\n",
      "|  6|Comedy|     0.0|\n",
      "|  5|Action|     1.0|\n",
      "|  2|Action|     1.0|\n",
      "|  4|Horror|     2.0|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "movie_data = spark.createDataFrame([(1, \"Comedy\"),\n",
    "(2, \"Action\"),\n",
    "(3, \"Comedy\"),\n",
    "(4, \"Horror\"),\n",
    "(5, \"Action\"),\n",
    "(6, \"Comedy\") ],[\"id\", \"genre\"])\n",
    "movieIndexer = StringIndexer(inputCol=\"genre\", outputCol=\"genreIdx\")\n",
    "# 首先拟合数据\n",
    "movieIndexModel = movieIndexer.fit(movie_data)\n",
    "# 使用返回的transformer来转换该数据\n",
    "indexedMovie = movieIndexModel.transform(movie_data)\n",
    "indexedMovie.show()\n",
    "\n",
    "# 显示结果\n",
    "print(movieIndexModel.labels)\n",
    "indexedMovie.orderBy(\"genreIdx\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comedy', 'Action', 'Horror']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieIndexModel.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用OneHotEncoderEstimator estimator\n",
    "### 注：OneHotEncoderEstimator从Spark 2.3.0 API中才出现，并且从Spark 3.0.0开始改名为OneHotEncoder，原来的OneHotEncoder会被删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+--------------+\n",
      "| id| genre|genreIdx|genreIdxVector|\n",
      "+---+------+--------+--------------+\n",
      "|  5|Action|     1.0| (2,[1],[1.0])|\n",
      "|  2|Action|     1.0| (2,[1],[1.0])|\n",
      "|  3|Comedy|     0.0| (2,[0],[1.0])|\n",
      "|  1|Comedy|     0.0| (2,[0],[1.0])|\n",
      "|  6|Comedy|     0.0| (2,[0],[1.0])|\n",
      "|  4|Horror|     2.0|     (2,[],[])|\n",
      "+---+------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoderEstimator estimator消费StringIndexer estimator的输出\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "# 输入列genreIdx是之前示例中StringIndex的输出列\n",
    "oneHotEncoderEst = OneHotEncoderEstimator(inputCols = [\"genreIdx\"], outputCols =[\"genreIdxVector\"],dropLast=True)\n",
    "# 指使indexedMovie数据（在上一个示例中产生的）\n",
    "oneHotEncoderModel = oneHotEncoderEst.fit(indexedMovie)\n",
    "oneHotEncoderVect = oneHotEncoderModel.transform(indexedMovie)\n",
    "print(oneHotEncoderVect.labels)\n",
    "oneHotEncoderVect .orderBy(\"genre\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 使用Word2Vec estimator来计算单词的嵌入和发现类似的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec具有两种模型，其一是 CBOW ，其思想是通过每个词的上下文窗口词词向量来预测中心词的词向量。\n",
    "#其二是 Skip-gram，其思想是通过每个中心词来预测其上下文窗口词，\n",
    "#并根据预测结果来修正中心词的词向量。两种方法示意图如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./CBOW.png\" width=\"30%\">,<img src=\"skip.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skip-gram.jpg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "documentDF = spark.createDataFrame([\n",
    "(\"Hi I heard about Spark\".split(\" \"), ),\n",
    "(\"I wish Java could use case classes\".split(\" \"), ),\n",
    "(\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "documentDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "|text                                      |result                                                           |\n",
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |[-0.0325182176893577,-0.06207826212048531,0.09105337858200074]   |\n",
      "|[I, wish, Java, could, use, case, classes]|[-0.011344701583896364,-0.06132613827607461,0.029776256232123287]|\n",
      "|[Logistic, regression, models, are, neat] |[-0.08718884875997901,-0.029027827456593516,0.02657611258327961] |\n",
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0,windowSize=5,inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------+\n",
      "|word      |vector                                                          |\n",
      "+----------+----------------------------------------------------------------+\n",
      "|heard     |[0.07805141806602478,0.10116440057754517,-0.08185699582099915]  |\n",
      "|are       |[-0.13309834897518158,-0.1130678728222847,-0.11082476377487183] |\n",
      "|neat      |[-0.14862976968288422,0.05695963650941849,0.10542261600494385]  |\n",
      "|classes   |[-0.0874999612569809,0.03785271197557449,0.14914149045944214]   |\n",
      "|I         |[0.15604113042354584,-0.15416713058948517,-0.15440209209918976] |\n",
      "|regression|[-0.1519547700881958,-0.09226707369089127,-0.10659107565879822] |\n",
      "|Logistic  |[-0.11443932354450226,0.14195924997329712,0.02488933876156807]  |\n",
      "|Spark     |[-0.0024795527569949627,0.07086249440908432,0.10906671732664108]|\n",
      "|could     |[0.1541176289319992,0.09678726643323898,-0.12757770717144012]   |\n",
      "|use       |[0.102830670773983,0.1281798928976059,0.027981573715806007]     |\n",
      "|Hi        |[0.05960903316736221,-0.1546497344970703,-0.049674905836582184] |\n",
      "|models    |[0.1298442929983139,0.04752953723073006,-0.1178247258067131]    |\n",
      "|case      |[-0.06380553543567657,-0.1121615618467331,0.06416858732700348]  |\n",
      "|about     |[0.12089268863201141,0.10763848572969437,-0.10426764190196991]  |\n",
      "|Java      |[0.011514928191900253,-0.08389845490455627,0.010653290897607803]|\n",
      "|wish      |[-0.0126124182716012,0.07262025773525238,-0.06183505058288574]  |\n",
      "+----------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在ml库中，Word2vec 的实现使用的是skip-gram模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用MinMaxScaler estimator来重新调节特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|empId|       features|\n",
      "+-----+---------------+\n",
      "|    1| [125400.0,5.3]|\n",
      "|    2| [179100.0,6.9]|\n",
      "|    3| [154770.0,5.2]|\n",
      "|    4|[199650.0,4.11]|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# 构造DataFrame\n",
    "employee_data = spark.createDataFrame(\n",
    "[\n",
    "(1, Vectors.dense(125400, 5.3)),\n",
    "(2, Vectors.dense(179100, 6.9)),\n",
    "(3, Vectors.dense(154770, 5.2)),\n",
    "(4, Vectors.dense(199650, 4.11))\n",
    "],[\"empId\", \"features\"])\n",
    "employee_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------------------------------+\n",
      "|empId|features       |scaledFeatures                           |\n",
      "+-----+---------------+-----------------------------------------+\n",
      "|1    |[125400.0,5.3] |[0.0,0.42652329749103923]                |\n",
      "|2    |[179100.0,6.9] |[0.7232323232323232,1.0]                 |\n",
      "|3    |[154770.0,5.2] |[0.39555555555555555,0.39068100358422936]|\n",
      "|4    |[199650.0,4.11]|[1.0,0.0]                                |\n",
      "+-----+---------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler estimator\n",
    "# x-min/max-min\n",
    "minMaxScaler = MinMaxScaler(min = 0.0, max=1.0, inputCol=\"features\",\n",
    "outputCol=\"scaledFeatures\")\n",
    "# 拟合数据，建立模型\n",
    "scalerModel = minMaxScaler.fit(employee_data)\n",
    "# 使用学习到的模型对数据集进行转换\n",
    "scaledData = scalerModel.transform(employee_data)\n",
    "# 输出特征缩放到的范围\n",
    "# 显示结果\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([199650.0, 6.9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerModel.originalMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用StandardScaler estimator标准化围绕均值0的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------------------------------------+\n",
      "|empId|features       |scaledFeatures                            |\n",
      "+-----+---------------+------------------------------------------+\n",
      "|1    |[125400.0,5.3] |[-1.2290717420781212,-0.06743742573177589]|\n",
      "|2    |[179100.0,6.9] |[0.4490658767775897,1.3248191055048937]   |\n",
      "|3    |[154770.0,5.2] |[-0.3112523404805006,-0.1544534589340674] |\n",
      "|4    |[199650.0,4.11]|[1.091258205781032,-1.102928220839048]    |\n",
      "+-----+---------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# 构造DataFrame\n",
    "employee_data = spark.createDataFrame([\n",
    "(1, Vectors.dense(125400, 5.3)),\n",
    "(2, Vectors.dense(179100, 6.9)),\n",
    "(3, Vectors.dense(154770, 5.2)),\n",
    "(4, Vectors.dense(199650, 4.11))\n",
    "],[\"empId\", \"features\"])\n",
    "# 将单位标准偏差设置为true并围绕平均值(withStd 是否为标准差 ,withMean 是否均值为0)\n",
    "standardScaler = StandardScaler(withStd = True, withMean = True, inputCol=\"features\",\n",
    "outputCol=\"scaledFeatures\")\n",
    "# 拟合数据，建立模型\n",
    "standardMode = standardScaler.fit(employee_data)\n",
    "# 使用学习到的模型对数据集进行转换\n",
    "standardData = standardMode.transform(employee_data)\n",
    "\n",
    "# 显示结果\n",
    "standardData.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
