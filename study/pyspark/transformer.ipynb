{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 字符串转成数字StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+\n",
      "|name|age|学历|\n",
      "+----+---+----+\n",
      "|李四| 15|高中|\n",
      "|王五| 19|本科|\n",
      "|张三| 18|硕士|\n",
      "| 赵6| 20|本科|\n",
      "+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-python\").getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "data=[\n",
    "    [\"李四\",15,\"高中\"],\n",
    "    [\"王五\",19,\"本科\"],\n",
    "    [\"张三\",18,\"硕士\"],\n",
    "    [\"赵6\",20,\"本科\"]\n",
    "]\n",
    "columns=[\"name\",\"age\",\"学历\"]\n",
    "df = spark.createDataFrame(data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['本科', '硕士', '高中']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = StringIndexer(inputCol=\"学历\", outputCol=\"学历_2\")#返回transfomer对象\n",
    "result = obj.fit(df)#返回transfomer对象,fit训练,并储存中间结果\n",
    "result.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+------+\n",
      "|name|age|学历|学历_2|\n",
      "+----+---+----+------+\n",
      "|李四| 15|高中|   2.0|\n",
      "|王五| 19|本科|   0.0|\n",
      "|张三| 18|硕士|   1.0|\n",
      "| 赵6| 20|本科|   0.0|\n",
      "+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = result.transform(df)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarizer 二值化:连续性\n",
    "离散型的二值化：StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+\n",
      "|  姓名|年龄|体重|学历|\n",
      "+------+----+----+----+\n",
      "|zhang3|  18|65.5|高中|\n",
      "|   li4|  19|50.2|本科|\n",
      "| wang5|  20|61.2|本科|\n",
      "| zhao6|  21|50.8|本科|\n",
      "|zheng7|  22|77.2|硕士|\n",
      "| zhou8|  22|80.7|硕士|\n",
      "+------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ['zhang3', 18, 65.5, '高中'],\n",
    "        ['li4',19, 50.2, '本科'],\n",
    "        ['wang5', 20,61.2, '本科'],\n",
    "        ['zhao6', 21, 50.8, '本科'],\n",
    "        ['zheng7', 22, 77.2, '硕士'],\n",
    "        ['zhou8',22, 80.7, '硕士'],\n",
    "    ], schema=['姓名','年龄','体重','学历']\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+-----------+\n",
      "|  姓名|年龄|体重|学历|体重_二值化|\n",
      "+------+----+----+----+-----------+\n",
      "|zhang3|  18|65.5|高中|        1.0|\n",
      "|   li4|  19|50.2|本科|        0.0|\n",
      "| wang5|  20|61.2|本科|        1.0|\n",
      "| zhao6|  21|50.8|本科|        0.0|\n",
      "|zheng7|  22|77.2|硕士|        1.0|\n",
      "| zhou8|  22|80.7|硕士|        1.0|\n",
      "+------+----+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obj = Binarizer(inputCol=\"体重\",outputCol=\"体重_二值化\",threshold=60)\n",
    "obj.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  使用Bucketizer transformer 分桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Bucketizer transformer是Binarizer的通用版本，它可以将列值转换成您所选择的桶。\n",
    "#控制桶的数量以及每个桶的取值范围的方法是通过指定一个double值数组的桶边界列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+---------+\n",
      "|  姓名|年龄|体重|学历|体重_分桶|\n",
      "+------+----+----+----+---------+\n",
      "|zhang3|  18|65.5|高中|      1.0|\n",
      "|   li4|  19|50.2|本科|      0.0|\n",
      "| wang5|  20|61.2|本科|      1.0|\n",
      "| zhao6|  21|50.8|本科|      0.0|\n",
      "|zheng7|  22|77.2|硕士|      2.0|\n",
      "| zhou8|  22|80.7|硕士|      2.0|\n",
      "+------+----+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [0.0, 55.0, 70.0, 90.0]\n",
    "obj = Bucketizer(inputCol=\"体重\",outputCol=\"体重_分桶\", splits=splits)\n",
    "obj.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  使用Tokenizer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|words                                                                   |\n",
      "+------------------------------------------------------------------------+\n",
      "|[spark, is, a, unified, data, data, analytics, engine, engine, engine]  |\n",
      "|[it, is, fun, to, work, with, spark]                                    |\n",
      "|[there, is, a, lot, of, exciting, sessions, at, upcoming, spark, summit]|\n",
      "|[mllib, transformer, estimator, evaluator, and, pipelines]              |\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Spark is a unified data data analytics engine engine engine\"),\n",
    "        (2, \"It is fun to work with Spark\"),\n",
    "        (3, \"There is a lot of exciting sessions at upcoming Spark summit\"),\n",
    "        (4, \"mllib transformer estimator evaluator and pipelines\")\n",
    "    ], \n",
    "    schema=[\"id\", \"line\"])\n",
    "\n",
    "obj = Tokenizer(inputCol=\"line\",outputCol=\"words\")\n",
    "df2 = obj.transform(df)\n",
    "df2.select('words').show(truncate=False) #默认会显示不全，加上truncate=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  使用StopWordsRemover transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|filtered                                                       |\n",
      "+---------------------------------------------------------------+\n",
      "|[spark, unified, data, data, analytics, engine, engine, engine]|\n",
      "|[fun, work, spark]                                             |\n",
      "|[lot, exciting, sessions, upcoming, spark, summit]             |\n",
      "|[mllib, transformer, estimator, evaluator, pipelines]          |\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用StopWordsRemover transformer来删除Tokenization示例中的单词中的英语停止词\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "#装载英文的停用词\n",
    "enStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "remover = StopWordsRemover(stopWords=enStopWords,inputCol=\"words\",outputCol=\"filtered\")\n",
    "# 来自上一示例中的tokenized\n",
    "df3 = remover.transform(df2)\n",
    "df3.select('filtered').show(truncate=False)\n",
    "#df2.select('words').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  使用HashingTF transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|TFOut                                                                    |\n",
      "+-------------------------------------------------------------------------+\n",
      "|(262144,[1461,7473,110213,160735,234657],[1.0,3.0,1.0,2.0,1.0])          |\n",
      "|(262144,[8443,34343,234657],[1.0,1.0,1.0])                               |\n",
      "|(262144,[3023,8916,14250,128231,166806,234657],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[91106,163638,165972,166537,191938],[1.0,1.0,1.0,1.0,1.0])       |\n",
      "+-------------------------------------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------------+\n",
      "|filtered                                                       |\n",
      "+---------------------------------------------------------------+\n",
      "|[spark, unified, data, data, analytics, engine, engine, engine]|\n",
      "|[fun, work, spark]                                             |\n",
      "|[lot, exciting, sessions, upcoming, spark, summit]             |\n",
      "|[mllib, transformer, estimator, evaluator, pipelines]          |\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#numFeatures为HashingTF类的成员变量默认为2^20，也就是hash的维数。\n",
    "#数字就是单词的哈希值，值就是单词的频数\n",
    "#哈希值的目的：计算单词重复出现的次数，并把单词用数字表示\n",
    "#接收词条的集合然后把这些集合转化成固定 长度的特征向量\n",
    "from pyspark.ml.feature import HashingTF\n",
    "tf = HashingTF(inputCol=\"filtered\", outputCol=\"TFOut\", numFeatures=262144)\n",
    "tfResult = tf.transform(df3)\n",
    "tfResult.select('TFOut').show(truncate=False) #哈希总数，哈希值，个数\n",
    "df3.select('filtered').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  使用VectorAssembler transformer来组合特征到一个Vecotr特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: bigint, temperature: double, on_time: boolean]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "arrival_features = spark.createDataFrame(\n",
    "    [\n",
    "        (18, 95.1, True),\n",
    "        (5, 65.7, True),\n",
    "        (15, 31.5,False),\n",
    "        (14, 40.5, False)\n",
    "    ], [\"hour\", \"temperature\",\"on_time\"])\n",
    "arrival_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------+---------------+\n",
      "|hour|temperature|on_time|features       |\n",
      "+----+-----------+-------+---------------+\n",
      "|18  |95.1       |true   |[18.0,95.1,1.0]|\n",
      "|5   |65.7       |true   |[5.0,65.7,1.0] |\n",
      "|15  |31.5       |false  |[15.0,31.5,0.0]|\n",
      "|14  |40.5       |false  |[14.0,40.5,0.0]|\n",
      "+----+-----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "                inputCols = [\"hour\",\"temperature\", \"on_time\"],\n",
    "                outputCol=\"features\"\n",
    "            )\n",
    "output = assembler.transform(arrival_features)\n",
    "output.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([18.0, 95.1, 1.0])),\n",
       " Row(features=DenseVector([5.0, 65.7, 1.0])),\n",
       " Row(features=DenseVector([15.0, 31.5, 0.0])),\n",
       " Row(features=DenseVector([14.0, 40.5, 0.0]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.select('features').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                line|\n",
      "+---+--------------------+\n",
      "|  1|Spark is a unifie...|\n",
      "|  2|It is fun to work...|\n",
      "|  3|There is a lot of...|\n",
      "|  4|mllib transformer...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  独热编码 OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+-------------+\n",
      "|  姓名|年龄|体重|学历|        年龄2|\n",
      "+------+----+----+----+-------------+\n",
      "|zhang3|   0|65.5|高中|(4,[0],[1.0])|\n",
      "|   li4|   1|50.2|本科|(4,[1],[1.0])|\n",
      "| wang5|   2|61.2|本科|(4,[2],[1.0])|\n",
      "| zhao6|   2|50.8|本科|(4,[2],[1.0])|\n",
      "|zheng7|   3|77.2|硕士|(4,[3],[1.0])|\n",
      "| zhou8|   3|80.7|硕士|(4,[3],[1.0])|\n",
      "+------+----+----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ['zhang3', 0, 65.5, '高中'],\n",
    "        ['li4',1, 50.2, '本科'],\n",
    "        ['wang5', 2,61.2, '本科'],\n",
    "        ['zhao6', 2, 50.8, '本科'],\n",
    "        ['zheng7', 3, 77.2, '硕士'],\n",
    "        ['zhou8',3, 80.7, '硕士'],\n",
    "    ], schema=['姓名','年龄','体重','学历']\n",
    ")\n",
    "\n",
    "obj = OneHotEncoder(inputCol='年龄', outputCol='年龄2',dropLast=False)\n",
    "df2 = obj.transform(df)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
